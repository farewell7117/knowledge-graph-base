{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.data import citation_graph as citegrh\n",
    "from dgl import DGLGraph\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from models import DGI, LogReg\n",
    "import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    print('len(features):', len(features))\n",
    "    labels = pd.get_dummies(data.labels).values\n",
    "    labels = torch.LongTensor(labels[np.newaxis])\n",
    "    print('len(labels):', len(labels))\n",
    "    mask = torch.ByteTensor(data.train_mask)\n",
    "    g = nx.adjacency_matrix(data.graph)\n",
    "    adj = process.normalize_adj(g + sp.eye(g.shape[0]))\n",
    "    if sparse:\n",
    "        sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    else:\n",
    "        adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "    return adj, sp_adj, features, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cora'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 10000\n",
    "patience = 50\n",
    "lr = 0.001\n",
    "l2_coef = 0.0\n",
    "drop_prob = 0.0\n",
    "hid_units = 256\n",
    "sparse = True\n",
    "nonlinearity = 'prelu' # special name to separate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(features): 2708\n",
      "len(labels): 1\n"
     ]
    }
   ],
   "source": [
    "adj, sp_adj, features, labels, mask = load_cora_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2708, 7])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = labels.shape[1]\n",
    "\n",
    "idx_train = torch.LongTensor(range(0, 140))\n",
    "idx_val = torch.LongTensor(range(140, 640))\n",
    "idx_test = torch.LongTensor(list(range(nb_classes-1000,nb_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Loss: tensor(0.6931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6588, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6406, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5758, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4188, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3077, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2103, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2066, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1627, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1631, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1462, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1307, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1339, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1299, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1222, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1221, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1163, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1156, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1084, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1035, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1089, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1066, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1061, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0943, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0901, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0950, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0950, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0846, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0882, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0928, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0792, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0788, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0792, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0817, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0739, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0683, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0688, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0751, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0615, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0598, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0688, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0687, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0588, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0688, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0597, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0627, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0612, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0597, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0564, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0691, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0612, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0519, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0588, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0651, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0501, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0537, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0581, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0501, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0564, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0581, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0453, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0518, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0424, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0642, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0462, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0542, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0452, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0424, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0597, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0492, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0484, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Early stopping!\n",
      "Loading 473th epoch\n",
      "0.78\n",
      "0.816\n",
      "0.7810000000000001\n",
      "0.8119999999999999\n",
      "0.7810000000000001\n",
      "0.816\n",
      "0.7890000000000001\n",
      "0.808\n",
      "0.786\n",
      "0.8140000000000001\n",
      "0.78\n",
      "0.8140000000000001\n",
      "0.779\n",
      "0.816\n",
      "0.7810000000000001\n",
      "0.804\n",
      "0.779\n",
      "0.8140000000000001\n",
      "0.782\n",
      "0.816\n",
      "0.7810000000000001\n",
      "0.806\n",
      "0.788\n",
      "0.81\n",
      "0.788\n",
      "0.802\n",
      "0.786\n",
      "0.8119999999999999\n",
      "0.782\n",
      "0.81\n",
      "0.785\n",
      "0.8140000000000001\n",
      "0.787\n",
      "0.81\n",
      "0.7810000000000001\n",
      "0.808\n",
      "0.7810000000000001\n",
      "0.806\n",
      "0.785\n",
      "0.816\n",
      "0.786\n",
      "0.806\n",
      "0.782\n",
      "0.82\n",
      "0.779\n",
      "0.804\n",
      "0.786\n",
      "0.816\n",
      "0.7840000000000001\n",
      "0.8119999999999999\n",
      "0.7840000000000001\n",
      "0.806\n",
      "0.79\n",
      "0.816\n",
      "0.7829999999999999\n",
      "0.802\n",
      "0.7840000000000001\n",
      "0.8119999999999999\n",
      "0.788\n",
      "0.816\n",
      "0.786\n",
      "0.816\n",
      "0.786\n",
      "0.804\n",
      "0.78\n",
      "0.81\n",
      "0.7840000000000001\n",
      "0.81\n",
      "0.786\n",
      "0.804\n",
      "0.778\n",
      "0.816\n",
      "0.788\n",
      "0.806\n",
      "0.786\n",
      "0.81\n",
      "0.785\n",
      "0.802\n",
      "0.778\n",
      "0.81\n",
      "0.7810000000000001\n",
      "0.8119999999999999\n",
      "0.786\n",
      "0.8119999999999999\n",
      "0.78\n",
      "0.8119999999999999\n",
      "0.782\n",
      "0.808\n",
      "0.7829999999999999\n",
      "0.82\n",
      "0.7829999999999999\n",
      "0.806\n",
      "0.782\n",
      "0.798\n",
      "0.7829999999999999\n",
      "0.8140000000000001\n",
      "0.7810000000000001\n",
      "0.822\n",
      "0.786\n",
      "0.808\n",
      "Average micro_f1_test: 0.7834399999999999\n",
      "Average micro_f1_val: 0.81068\n",
      "CPU times: user 28.9 s, sys: 15.9 s, total: 44.8 s\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DGI(ft_size, hid_units, nonlinearity)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA')\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    if sparse:\n",
    "        sp_adj = sp_adj.cuda()\n",
    "    else:\n",
    "        adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "b_xent = nn.BCEWithLogitsLoss()\n",
    "xent = nn.CrossEntropyLoss()\n",
    "cnt_wait = 0\n",
    "best = 1e9\n",
    "best_t = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    idx = np.random.permutation(nb_nodes)\n",
    "    shuf_fts = features[idx, :] #add first dimension = 1\n",
    "\n",
    "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
    "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
    "    lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        shuf_fts = shuf_fts.cuda()\n",
    "        lbl = lbl.cuda()\n",
    "    \n",
    "    logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "\n",
    "    loss = b_xent(logits, lbl)\n",
    "\n",
    "    print('Loss:', loss)\n",
    "\n",
    "    if loss < best:\n",
    "        best = loss\n",
    "        best_t = epoch\n",
    "        cnt_wait = 0\n",
    "        torch.save(model.state_dict(), 'best_dgi.pkl')\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "\n",
    "    if cnt_wait == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
    "\n",
    "embeds, _ = model.embed(features, sp_adj if sparse else adj, sparse, None)\n",
    "train_embs = embeds[0, idx_train]\n",
    "val_embs = embeds[0, idx_val]\n",
    "test_embs = embeds[0, idx_test]\n",
    "\n",
    "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
    "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
    "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
    "\n",
    "tot = torch.zeros(1)\n",
    "tot = tot.cuda()\n",
    "\n",
    "tot1 = torch.zeros(1)\n",
    "tot1 = tot.cuda()\n",
    "\n",
    "Micro_f1 = []\n",
    "Micro_f1_val = []\n",
    "\n",
    "for _ in range(50):\n",
    "    log = LogReg(hid_units, nb_classes)\n",
    "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
    "    log.cuda()\n",
    "\n",
    "    pat_steps = 0\n",
    "    best_acc = torch.zeros(1)\n",
    "    best_acc = best_acc.cuda()\n",
    "    for _ in range(100):\n",
    "        log.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = log(train_embs)\n",
    "        loss = xent(logits, train_lbls)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    logits = log(test_embs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "    micro_f1 = f1_score(test_lbls.data.cpu().numpy(), preds.data.cpu().numpy(), average=\"micro\")\n",
    "    Micro_f1.append(micro_f1)\n",
    "    print(micro_f1)\n",
    "\n",
    "    \n",
    "    logits1 = log(val_embs)\n",
    "    preds1 = torch.argmax(logits1, dim=1)\n",
    "    micro_f1_val = f1_score(val_lbls.data.cpu().numpy(), preds1.data.cpu().numpy(), average=\"micro\")\n",
    "    Micro_f1_val.append(micro_f1_val)\n",
    "    print(micro_f1_val)\n",
    "\n",
    "print('Average micro_f1_test:', np.mean(Micro_f1))\n",
    "print('Average micro_f1_val:', np.mean(Micro_f1_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
