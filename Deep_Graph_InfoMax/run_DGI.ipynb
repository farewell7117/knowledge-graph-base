{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.data import citation_graph as citegrh\n",
    "from dgl import DGLGraph\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from models import DGI, LogReg\n",
    "import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    print('len(features):', len(features))\n",
    "    labels = pd.get_dummies(data.labels).values\n",
    "    labels = torch.LongTensor(labels[np.newaxis])\n",
    "    print('len(labels):', len(labels))\n",
    "    mask = torch.ByteTensor(data.train_mask)\n",
    "    g = nx.adjacency_matrix(data.graph)\n",
    "    adj = process.normalize_adj(g + sp.eye(g.shape[0]))\n",
    "    if sparse:\n",
    "        sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    else:\n",
    "        adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "    return adj, sp_adj, features, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cora'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 10000\n",
    "patience = 50\n",
    "lr = 0.001\n",
    "l2_coef = 0.0\n",
    "drop_prob = 0.0\n",
    "hid_units = 256\n",
    "sparse = True\n",
    "nonlinearity = 'prelu' # special name to separate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(features): 2708\n",
      "len(labels): 1\n"
     ]
    }
   ],
   "source": [
    "adj, sp_adj, features, labels, mask = load_cora_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2708, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = labels.shape[1]\n",
    "\n",
    "idx_train = torch.LongTensor(range(0, 140))\n",
    "idx_val = torch.LongTensor(range(140, 640))\n",
    "idx_test = torch.LongTensor(list(range(nb_classes-1000,nb_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n",
      "Loss: tensor(0.6932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6277, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6225, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.6047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5603, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.5053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4832, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.4009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3901, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3807, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3060, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.3006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2300, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2090, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.2006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1739, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1686, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1691, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1547, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1441, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1405, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1416, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1258, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1100, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1163, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1090, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1193, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1061, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1139, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1044, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0993, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.1025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0969, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0922, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0889, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0882, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0807, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0876, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0846, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0774, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0779, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0762, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0683, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0762, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0688, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0644, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0650, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0599, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0691, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0650, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0651, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0565, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0550, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0543, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0581, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0519, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0519, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0492, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0581, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0453, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Loss: tensor(0.0576, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Early stopping!\n",
      "Loading 408th epoch\n",
      "0.787\n",
      "0.7890000000000001\n",
      "0.791\n",
      "0.79\n",
      "0.791\n",
      "0.79\n",
      "0.7840000000000001\n",
      "0.7890000000000001\n",
      "0.788\n",
      "0.786\n",
      "0.787\n",
      "0.7890000000000001\n",
      "0.7890000000000001\n",
      "0.786\n",
      "0.786\n",
      "0.788\n",
      "0.7890000000000001\n",
      "0.788\n",
      "0.786\n",
      "0.786\n",
      "0.7940000000000002\n",
      "0.7940000000000002\n",
      "0.7890000000000001\n",
      "0.788\n",
      "0.788\n",
      "0.786\n",
      "0.792\n",
      "0.79\n",
      "0.796\n",
      "0.787\n",
      "0.7890000000000001\n",
      "0.793\n",
      "0.787\n",
      "0.782\n",
      "0.793\n",
      "0.7890000000000001\n",
      "0.788\n",
      "0.791\n",
      "0.791\n",
      "0.792\n",
      "0.793\n",
      "0.788\n",
      "0.791\n",
      "0.793\n",
      "0.788\n",
      "0.787\n",
      "0.791\n",
      "0.7940000000000002\n",
      "0.793\n",
      "0.793\n",
      "Average micro_f1: tensor([0.7894], device='cuda:0')\n",
      "CPU times: user 26.1 s, sys: 14.5 s, total: 40.6 s\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DGI(ft_size, hid_units, nonlinearity)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA')\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    if sparse:\n",
    "        sp_adj = sp_adj.cuda()\n",
    "    else:\n",
    "        adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "b_xent = nn.BCEWithLogitsLoss()\n",
    "xent = nn.CrossEntropyLoss()\n",
    "cnt_wait = 0\n",
    "best = 1e9\n",
    "best_t = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    idx = np.random.permutation(nb_nodes)\n",
    "    shuf_fts = features[idx, :] #add first dimension = 1\n",
    "\n",
    "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
    "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
    "    lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        shuf_fts = shuf_fts.cuda()\n",
    "        lbl = lbl.cuda()\n",
    "    \n",
    "    logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
    "\n",
    "    loss = b_xent(logits, lbl)\n",
    "\n",
    "    print('Loss:', loss)\n",
    "\n",
    "    if loss < best:\n",
    "        best = loss\n",
    "        best_t = epoch\n",
    "        cnt_wait = 0\n",
    "        torch.save(model.state_dict(), 'best_dgi_1.pkl')\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "\n",
    "    if cnt_wait == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_dgi_1.pkl'))\n",
    "\n",
    "embeds, _ = model.embed(features, sp_adj if sparse else adj, sparse, None)\n",
    "train_embs = embeds[0, idx_train]\n",
    "val_embs = embeds[0, idx_val]\n",
    "test_embs = embeds[0, idx_test]\n",
    "\n",
    "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
    "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
    "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
    "\n",
    "tot = torch.zeros(1)\n",
    "tot = tot.cuda()\n",
    "\n",
    "\n",
    "Micro_f1 = []\n",
    "\n",
    "for _ in range(50):\n",
    "    log = LogReg(hid_units, nb_classes)\n",
    "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
    "    log.cuda()\n",
    "\n",
    "    pat_steps = 0\n",
    "    best_acc = torch.zeros(1)\n",
    "    best_acc = best_acc.cuda()\n",
    "    for _ in range(100):\n",
    "        log.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = log(train_embs)\n",
    "        loss = xent(logits, train_lbls)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    logits = log(test_embs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "    micro_f1 = f1_score(test_lbls.data.cpu().numpy(), preds.data.cpu().numpy(), average=\"micro\")\n",
    "    Micro_f1.append(micro_f1)\n",
    "    print(micro_f1)\n",
    "    tot += micro_f1\n",
    "\n",
    "print('Average micro_f1:', tot / 50)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
